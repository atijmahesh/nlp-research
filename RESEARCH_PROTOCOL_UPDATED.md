# Investigating the Impact of Constraint-Based Control Strategies on Gender-Stereotype Language and Fluency in Large Language Models

## Abstract

This study evaluates how five control strategies‚Äîprompt-only, DFA-based Ctrl‚ÄëG decoding, generate-and-filter, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Iterative Nullspace Projection (INLP)‚Äîaffect gender-stereotyped language in neutral occupation contexts. We measure constraint compliance, lexical diversity, and fluency, with a novel focus on how different learning paradigms (supervised learning vs. preference learning vs. linear projection) influence bias mitigation effectiveness. Our findings reveal that supervised fine-tuning achieves near-perfect compliance (99.87%) while preference-based learning catastrophically fails (4.53%), providing strong evidence that explicit positive examples are essential for compositional constraint satisfaction.

## 1. Introduction

Gender stereotypes in language models can reinforce societal biases (Rudinger et al., 2018). Recent work explores control strategies to mitigate these biases: Dathathri et al. (2020) introduced controlled generation via DFA constraints (Ctrl‚ÄëG); Ravfogel et al. (2022) applied INLP for projection-based debiasing; Rafailov et al. (2023) proposed Direct Preference Optimization (DPO) as a simpler alternative to RLHF; and supervised fine-tuning has shown promise for task-specific adaptation. Yet, the comparative effectiveness of these methods for bias mitigation and their impact on fluency remain underexplored. This study systematically compares six control strategies to understand their trade-offs.

## 2. Research Question

How do prompt-only, DFA-based Ctrl‚ÄëG decoding, generate-and-filter, and lightweight debiasing fine-tunes (SFT, DPO, INLP) alter gender-stereotyped language across neutral occupations, in terms of:

- **Constraint compliance**: Presence of both agentic AND communal terms
- **Lexical diversity**: Shannon entropy over synonym frequencies
- **Fluency**: Perplexity under a reference LM
- **Training efficiency**: GPU time and memory requirements

## 3. Materials

### Occupations
20 prefixes adapted from Winogender Schemas (Rudinger et al., 2018):
- architect, artist, barista, chef, counselor, doctor, electrician, engineer, journalist, lawyer, mechanic, nurse, pharmacist, photographer, pilot, plumber, scientist, teacher, salesperson, writer

**Train/Val Split:**
- **Training** (15): architect, artist, chef, counselor, doctor, engineer, journalist, lawyer, nurse, pharmacist, photographer, pilot, scientist, teacher, writer
- **Validation** (5): barista, electrician, mechanic, plumber, salesperson

### Synonym Lists
Gaucher et al. (2011), 10 terms each:

**Agentic**: ambitious, assertive, bold, confident, decisive, independent, self-reliant, competitive, adventurous, dominant

**Communal**: accommodating, caring, cooperative, empathetic, friendly, nurturing, supportive, compassionate, helpful, loyal

### Model Roster

**Baseline Models:**
- Prompt-only: GPT‚Äë4o, LLaMA-4-Scout, LLaMA-3.3-70B
- Ctrl‚ÄëG decoding: GPT‚Äë2 Ctrl‚ÄëG, LLaMA-4.0+HMM, LLaMA-3.1-8B+HMM
- Generate-and-filter: Applied to baseline models

**Debiasing Fine-Tunes (All use LLaMA-3.1-8B-Instruct):**
- **SFT**: Supervised Fine-Tuning with LoRA
- **DPO**: Direct Preference Optimization with LoRA
- **INLP**: Iterative Nullspace Projection (post-hoc)

## 4. Prompt Design

**Baseline prompt** (consistent across all methods):
```
Complete the following sentence in natural, coherent English (8‚Äì15 words long):
"The [OCCUPATION] was"
```

## 5. Methods

### 5.1 Prompt-Only Baseline
- Generate 5 completions per occupation per model
- Enforce 8‚Äì15 word lengths via regeneration

### 5.2 Ctrl‚ÄëG Decoding
- Build DFA over agentic + communal + length constraints
- Generate 500 samples per occupation
- Hard constraints ensure compliance

### 5.3 Generate-and-Filter
- Generate 100 raw samples per occupation
- Retain outputs with ‚â•1 agentic OR communal term
- Cap at 250 retained samples per occupation

### 5.4 Supervised Fine-Tuning (SFT) with LoRA

**Training Data:** 750 programmatic examples (50 per occupation, 18 templates)
- Each example contains ‚â•1 agentic AND ‚â•1 communal term
- Templates vary syntactic structure to prevent memorization

**Method:**
- Fine-tune LLaMA-3.1-8B-Instruct with LoRA (rank=8, alpha=16)
- Target modules: q_proj, k_proj, v_proj, o_proj
- Trainable parameters: ~0.08% (6.8M/8B)

**Hyperparameters:**
- Learning rate: 2e-4
- Batch size: 4 (per device)
- Gradient accumulation: 4 (effective batch size: 16)
- Epochs: 3
- bf16 precision + gradient checkpointing
- Max new tokens: 64, temperature: 1.0, top_p: 0.95

**Training:**
- Hardware: NVIDIA RTX A6000 (48GB)
- Time: ~3 hours per seed
- Memory: ~10-15GB
- Seeds: 42, 123, 456 (for reproducibility)

**Results:**
- Seed 42: 99.7% balanced (4983/5000)
- Seed 123: 99.9% balanced (4993/5000)
- Seed 456: 100.0% balanced (5000/5000)
- **Mean: 99.87% ¬± 0.15%**

### 5.5 Direct Preference Optimization (DPO) with LoRA

**Training Data:** 750 preference pairs
- **Chosen**: Balanced outputs (agentic + communal)
- **Rejected**: Unbalanced outputs (only agentic OR communal)

**Method:**
- Fine-tune LLaMA-3.1-8B-Instruct with LoRA (rank=8, alpha=16)
- Use frozen reference model for KL penalty
- Optimize preference classification loss

**Hyperparameters:**
- Learning rate: 5e-5
- Batch size: 1 (per device, dual-model memory constraint)
- Gradient accumulation: 16 (effective batch size: 16)
- Beta (DPO temperature): 0.1
- Epochs: 3
- bf16 precision + gradient checkpointing

**Training:**
- Hardware: NVIDIA RTX A6000 (48GB)
- Time: ~3-4 hours per seed (slower due to dual models)
- Memory: ~12-18GB (policy + reference models)
- Seeds: 42, 123, 456

**Results:**
- Seed 42: 4.6% balanced (230/5000)
- Seed 123: 3.7% balanced (183/5000)
- Seed 456: 5.3% balanced (263/5000)
- **Mean: 4.53% ¬± 0.82%**

**Analysis:** DPO catastrophically failed because binary preferences ("balanced > unbalanced") do not provide sufficient information for learning compositional constraints. The model learns relative ordering but not the specific requirement of including BOTH term types.

### 5.6 Iterative Nullspace Projection (INLP)

**Training Data:** 39 gendered word pairs
- Pronouns: he/she, him/her, his/hers, himself/herself
- Family: man/woman, father/mother, son/daughter, brother/sister, etc.
- Professional: actor/actress, waiter/waitress, businessman/businesswoman, etc.
- Titles: mr/mrs, sir/madam, king/queen, prince/princess, etc.

**Method:**
1. Extract embeddings for gendered words at layer -1 (last hidden layer)
2. Train linear classifier to predict gender from embeddings
3. Extract decision boundary direction (classifier weight vector)
4. Project embeddings onto orthogonal complement (remove direction)
5. Iterate until no gender signal remains (early stopping)

**Hyperparameters:**
- Max iterations: 300
- Classifier: Logistic Regression (C=0.01 for regularization)
- Layer: -1 (last hidden layer, 4096-dim)
- Early stopping: When classifier achieves near-zero norm

**Training:**
- Hardware: NVIDIA RTX A6000 (48GB)
- Time: **~20 seconds per seed** (10√ó faster than SFT/DPO!)
- Memory: ~10-15GB (single model, no gradients)
- Seeds: 42 (26 directions), 123 (23 directions), 456 (24 directions)

**Results:** TBD (generation in progress)

## 6. Investigation of Fluency Impact

We quantify how constraint severity affects fluency by:
- Measuring average perplexity under GPT-2 reference LM
- Comparing fluency across methods
- Correlating compliance rates with perplexity
- Analyzing trade-offs between constraint satisfaction and naturalness

## 7. Metrics & Analysis

**Primary Metrics:**
- **Constraint compliance**: % samples with ‚â•1 agentic AND ‚â•1 communal term
- **Lexical diversity**: Shannon entropy over synonym frequencies
- **Path diversity**: Unique (agentic, communal) pairs / total samples
- **Fluency**: Average perplexity under reference LM
- **Training efficiency**: GPU hours and memory per method

**Statistical Tests:**
- Wilcoxon signed-rank for paired comparisons
- Cohen's d for effect sizes
- Bootstrap confidence intervals for cross-seed variability

## 8. Preliminary Results

### Constraint Compliance

| Method | Mean Balanced | Std Dev | Status |
|--------|---------------|---------|--------|
| SFT | **99.87%** | 0.15% | ‚úÖ Near-perfect |
| DPO | **4.53%** | 0.82% | ‚ùå Catastrophic failure |
| INLP | TBD | TBD | üîÑ In progress |
| Prompt-Only | ~5-10% | - | Baseline |
| Gen-Filter | ~30-50% | - | Moderate |
| Ctrl-G | ~100% | - | Hard constraints |

### Training Efficiency

| Method | GPU Time | Memory | Trainable Params |
|--------|----------|--------|------------------|
| SFT | ~3h/seed | 10-15GB | 0.08% (LoRA) |
| DPO | ~3-4h/seed | 12-18GB | 0.08% (LoRA) |
| INLP | **~20s/seed** | 10-15GB | 0% (post-hoc) |

### Key Finding

**Supervised learning dramatically outperforms preference learning** for bias mitigation:
- SFT achieved 99.87% compliance by learning from explicit balanced examples
- DPO achieved only 4.53% compliance despite learning from preference pairs
- This reveals that compositional constraints require explicit positive supervision, not just relative preferences

**Hypothesis:** DPO's failure stems from its implicit learning paradigm. The model learns "balanced is better than unbalanced" but not "what makes something balanced." For compositional constraints requiring BOTH properties simultaneously, explicit examples are essential.

## 9. Novel Contributions

1. **First systematic comparison** of supervised learning (SFT) vs. preference learning (DPO) for bias mitigation
2. **Strong negative result**: DPO's 4.53% vs. SFT's 99.87% shows preference learning is insufficient for compositional constraints
3. **Efficiency analysis**: INLP is 10√ó faster than fine-tuning but effectiveness TBD
4. **Unified protocol**: Identical prompts, model, and hyperparameters enable clean comparisons
5. **Reproducibility**: Multiple seeds (42, 123, 456) demonstrate consistency

## 10. Deliverables

- ‚úÖ Cleaned, annotated datasets (15,000 SFT + 15,000 DPO + 15,000 INLP completions)
- ‚è≥ Comprehensive metrics across all methods
- ‚è≥ Statistical analysis and visualizations
- ‚è≥ Manuscript draft with negative result emphasis
- ‚úÖ Public repository with all scripts and data

## References

- Dathathri, S., et al. (2020). Plug and Play Language Models. *ICLR*.
- Gaucher, D., et al. (2011). Evidence That Gendered Wording in Job Advertisements Exists. *JPSP*, 101(1), 109‚Äì128.
- Hu, E., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. *ICLR 2022*.
- Rafailov, R., et al. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. *arXiv:2305.18290*.
- Ravfogel, S., et al. (2022). Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection. *EMNLP*.
- Rudinger, R., et al. (2018). Gender Bias in Coreference Resolution: The Winogender Schemas. *arXiv:1804.09301*.

## Appendix: Implementation Details

### LoRA Configuration
```python
LoraConfig(
    r=8,                    # Rank
    lora_alpha=16,          # Scaling factor
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```

### SFT Example Templates
1. "The {occ} was {agentic} and {communal} in their work."
2. "Known for being {agentic} yet {communal}, the {occ} excelled."
3. "The {agentic} and {communal} {occ} built strong relationships."
...
(18 templates total, ensuring syntactic diversity)

### DPO Preference Pair Example
```
Prompt: "The architect was"
Chosen: "ambitious and caring in their innovative designs"
Rejected: "ambitious and driven to create bold structures"
```

### INLP Early Stopping
- Iteration stops when classifier norm < 1e-8
- Typical convergence: 23-26 iterations (~20 seconds)
- Much faster than theoretical 300 iterations due to effective debiasing

